import asyncio
import os
import shutil
import subprocess
from datetime import datetime
from pathlib import Path
from typing import Iterable, Optional

from sqlmodel import Session

from ..config import settings
from ..db import engine
from ..models import Pipeline, Run, RunStatus
from .events import bus


# ----------------------------
# Helpers: SSE events & logging
# ----------------------------

# Global dict to store log file paths for each run
_log_files = {}

def _sanitize_name(name: str) -> str:
    """Sanitize pipeline name for use in file paths and Docker."""
    return name.lower().replace(" ", "-").replace("_", "-")

def _get_log_file(pipeline_name: str, run_id: int) -> Path:
    """Get or create log file path for a run."""
    if run_id not in _log_files:
        sanitized_name = _sanitize_name(pipeline_name)
        log_dir = Path.home() / ".cicd" / "workspaces" / sanitized_name / "logs"
        log_dir.mkdir(parents=True, exist_ok=True)
        # Utiliser le nom du pipeline comme nom de fichier
        log_file = log_dir / f"{sanitized_name}.log"
        _log_files[run_id] = log_file
        # Initialize log file with header (√©crase le fichier pr√©c√©dent)
        with open(log_file, 'w') as f:
            f.write(f"=== Pipeline: {pipeline_name} | Run {run_id} | {datetime.utcnow().isoformat()} ===\n\n")
    return _log_files[run_id]

def _write_to_log(log_file: Path, message: str):
    """Write a message to the log file."""
    # Ensure parent directory exists
    log_file.parent.mkdir(parents=True, exist_ok=True)
    with open(log_file, 'a') as f:
        f.write(f"{message}\n")

async def _emit(run_id: int, evt: dict, pipeline_name: Optional[str] = None):
    """Publish event to SSE bus."""
    await bus.publish(run_id, evt)

async def _step_start(run_id: int, step: str, pipeline_name: Optional[str] = None):
    if pipeline_name:
        log_file = _get_log_file(pipeline_name, run_id)
        _write_to_log(log_file, f"\n>>> STEP: {step}")
    await _emit(run_id, {"type": "step_start", "step": step})

async def _step_ok(run_id: int, step: str, pipeline_name: Optional[str] = None):
    if pipeline_name:
        log_file = _get_log_file(pipeline_name, run_id)
        _write_to_log(log_file, f"‚úì STEP COMPLETED: {step}")
    await _emit(run_id, {"type": "step_success", "step": step})

async def _log(run_id: int, step: str, line: str, pipeline_name: Optional[str] = None):
    message = line.rstrip("\n")
    if pipeline_name:
        log_file = _get_log_file(pipeline_name, run_id)
        _write_to_log(log_file, f"[{step}] {message}")
    await _emit(run_id, {"type": "log", "step": step, "message": message})


# ----------------------------
# Helpers: process execution
# ----------------------------

def _run_cmd(cmd: list[str], cwd: Optional[str] = None) -> Iterable[str]:
    """Run command and yield merged stdout/stderr lines. Raises RuntimeError on non-zero exit."""
    p = subprocess.Popen(
        cmd,
        cwd=cwd,
        stdout=subprocess.PIPE,
        stderr=subprocess.STDOUT,
        text=True,
        bufsize=1,
    )
    assert p.stdout is not None
    for line in p.stdout:
        yield line
    rc = p.wait()
    if rc != 0:
        raise RuntimeError(f"Command failed ({rc}): {' '.join(cmd)}")


def _workspace(pipeline_name: str) -> Path:
    """Get workspace path for a pipeline. Workspace is recreated on each run."""
    base = Path.home() / ".cicd" / "workspaces"
    base.mkdir(parents=True, exist_ok=True)
    sanitized_name = _sanitize_name(pipeline_name)
    return base / sanitized_name


def _git_checkout(repo_url: str, branch: str, ws: Path) -> None:
    """Ensure workspace contains up-to-date checkout of repo@branch."""
    # Supprimer compl√®tement le workspace s'il existe (pour repartir propre √† chaque run)
    if ws.exists():
        shutil.rmtree(ws)
    
    # Cr√©er le dossier parent
    ws.parent.mkdir(parents=True, exist_ok=True)
    
    # Clone frais
    for _ in _run_cmd(["git", "clone", "--branch", branch, "--single-branch", repo_url, str(ws)]):
        pass


def _ssh_exec(user: str, host: str, port: int, remote_cmd: str) -> Iterable[str]:
    return _run_cmd(["ssh", "-p", str(port), f"{user}@{host}", remote_cmd])


def _docker_save_and_load_over_ssh(user: str, host: str, port: int, image_tag: str) -> Iterable[str]:
    """
    sudo docker save <image_tag> | ssh user@host "docker load"
    Returns output lines for real-time logging.
    Uses sudo locally for docker save, but not on remote (docker has permissions there).
    """
    # 1. Save Docker image to stdout (needs sudo locally)
    save = subprocess.Popen(
        ["sudo", "docker", "save", image_tag],
        stdout=subprocess.PIPE,
        stderr=subprocess.PIPE,
    )
    assert save.stdout is not None

    # 2. Load via SSH with compression and keep-alive (no sudo needed on remote)
    load = subprocess.Popen(
        [
            "ssh",
            "-p", str(port),
            "-o", "ServerAliveInterval=30",   # Keep-alive every 30s
            "-o", "ServerAliveCountMax=10",    # Max 10 retries
            "-o", "Compression=yes",           # Compress transfer
            "-o", "TCPKeepAlive=yes",          # TCP keep-alive
            f"{user}@{host}",
            "docker load"  
        ],
        stdin=save.stdout,
        stdout=subprocess.PIPE,
        stderr=subprocess.STDOUT,
        text=True,
        bufsize=1,
    )
    save.stdout.close()

    output = []
    assert load.stdout is not None
    
    # Yield lines for real-time logging
    for line in load.stdout:
        output.append(line)
        yield line.rstrip("\n")

    # Wait for both processes
    load_rc = load.wait()
    save_rc = save.wait()
    
    # Check for docker save errors
    if save_rc != 0:
        save_stderr = save.stderr.read().decode() if save.stderr else ""
        raise RuntimeError(f"docker save failed (exit code {save_rc}): {save_stderr}")
    
    # Check for docker load errors
    if load_rc != 0:
        raise RuntimeError(f"docker load over ssh failed (exit code {load_rc}):\n" + "".join(output))


def _healthcheck(url: str, timeout_sec: int = 10, retries: int = 30, delay_sec: float = 2.0) -> tuple[bool, str]:
    """
    Simple HTTP GET healthcheck.
    Returns (success, message) tuple.
    """
    import urllib.request
    import time

    last_error = ""
    for attempt in range(1, retries + 1):
        try:
            with urllib.request.urlopen(url, timeout=timeout_sec) as r:
                if 200 <= r.status < 300:
                    return True, f"Success on attempt {attempt}/{retries} (status {r.status})"
                else:
                    last_error = f"HTTP {r.status}"
        except Exception as e:
            last_error = str(e)
        
        if attempt < retries:
            time.sleep(delay_sec)
    
    return False, f"Failed after {retries} attempts. Last error: {last_error}"


# ----------------------------
# Helpers: Rollback & State Management
# ----------------------------

def _save_previous_commit(ws: Path) -> Optional[str]:
    """Save the current git commit hash (before updating to new version)."""
    try:
        result = subprocess.run(
            ["git", "rev-parse", "HEAD"],
            cwd=str(ws),
            capture_output=True,
            text=True,
            check=True
        )
        return result.stdout.strip()
    except Exception:
        return None


async def _rollback_to_previous(
    run_id: int,
    user: str,
    host: str,
    port: int,
    container_name: str,
    ws: Path,
    previous_commit: str,
    pipeline_name: Optional[str] = None,
):
    """Rollback: checkout previous commit and redeploy."""
    step = "rollback"
    await _step_start(run_id, step, pipeline_name)
    
    if not previous_commit:
        await _log(run_id, step, "No previous commit available, cannot rollback", pipeline_name)
        await _step_ok(run_id, step, pipeline_name)
        return
    
    try:
        # Stop and remove the failed new container
        await _log(run_id, step, f"Stopping failed container: {container_name}", pipeline_name)
        stop_cmd = f"docker ps -q --filter 'name=^{container_name}$' | xargs -r docker stop 2>/dev/null || true"
        for line in _ssh_exec(user, host, port, stop_cmd):
            pass
        
        remove_cmd = f"docker ps -aq --filter 'name=^{container_name}$' | xargs -r docker rm -f 2>/dev/null || true"
        for line in _ssh_exec(user, host, port, remove_cmd):
            pass
        
        # Checkout previous commit
        await _log(run_id, step, f"Checking out previous commit: {previous_commit}", pipeline_name)
        for line in _run_cmd(["git", "checkout", previous_commit], cwd=str(ws)):
            # Skip verbose git messages
            if line.strip() and "HEAD is now at" in line:
                await _log(run_id, step, line.strip(), pipeline_name)
        
        await _log(run_id, step, "‚úÖ Rollback checkout completed.", pipeline_name)
        await _step_ok(run_id, step, pipeline_name)
        
    except Exception as e:
        await _log(run_id, step, f"‚ö†Ô∏è Rollback checkout failed: {e}", pipeline_name)


# ----------------------------
# Main runner
# ----------------------------

async def run_real_pipeline(run_id: int):
    """
    Real pipeline runner - version simplifi√©e:
      1) Clone le repo Git
      2) Build with Maven (./mvnw -B clean compile)
      3) Run tests (./mvnw -B test)
      4) Docker build (si tests passent)
      5) Ship image via SSH
      6) Docker run on remote
      7) Healthcheck
    """
    # Configuration SSH en dur
    DEPLOY_USER = "mohamed"
    DEPLOY_HOST = "188.166.77.14"
    DEPLOY_PORT = 22
    
    with Session(engine) as session:
        run = session.get(Run, run_id)
        if not run:
            return

        pipeline = session.get(Pipeline, run.pipeline_id)
        if not pipeline:
            return

        # Mark run as running
        run.status = RunStatus.running
        session.add(run)
        session.commit()

        # Sanitize pipeline name for Docker (no spaces, lowercase)
        sanitized_name = _sanitize_name(pipeline.name)
        image_tag = f"{sanitized_name}:run-{run_id}"
        container_name = sanitized_name
        
        # Initialize workspace
        ws = _workspace(pipeline.name)
        
        # Save previous commit BEFORE checkout
        previous_commit = None
        if ws.exists():
            previous_commit = _save_previous_commit(ws)
            if previous_commit:
                await _emit(run_id, {"type": "log", "step": "init", "message": f"üìå Saved previous commit: {previous_commit}"}, pipeline.name)

        try:
            await _emit(run_id, {"type": "run_start"}, pipeline.name)

            # STEP: checkout
            step = "checkout"
            await _step_start(run_id, step, pipeline.name)
            ws = _workspace(pipeline.name)
            await _log(run_id, step, f"Workspace: {ws}", pipeline.name)
            await _log(run_id, step, f"Cloning {pipeline.github_url} ({pipeline.branch})", pipeline.name)
            _git_checkout(pipeline.github_url, pipeline.branch, ws)
            await _step_ok(run_id, step, pipeline.name)

            # STEP: tests Maven
            step = "maven_tests"
            await _step_start(run_id, step, pipeline.name)
            
            demo_dir = ws / "demo"
            if not demo_dir.exists():
                await _log(run_id, step, "No demo directory found, skipping tests", pipeline.name)
                await _step_ok(run_id, step, pipeline.name)
            else:
                # Build with Maven
                await _log(run_id, step, "Building with Maven (./mvnw -B clean compile)...", pipeline.name)
                for line in _run_cmd(["./mvnw", "-B", "clean", "compile"], cwd=str(demo_dir)):
                    await _log(run_id, step, line, pipeline.name)
                
                # Run tests
                await _log(run_id, step, "Running tests (./mvnw -B test)...", pipeline.name)
                for line in _run_cmd(["./mvnw", "-B", "test"], cwd=str(demo_dir)):
                    await _log(run_id, step, line, pipeline.name)
                
                await _log(run_id, step, "‚úÖ Tests passed successfully!", pipeline.name)
                await _step_ok(run_id, step, pipeline.name)

            # STEP: sonarcloud analysis
            step = "sonarcloud_analysis"
            await _step_start(run_id, step, pipeline.name)
            
            if not demo_dir.exists():
                await _log(run_id, step, "No demo directory found, skipping SonarCloud", pipeline.name)
                await _step_ok(run_id, step, pipeline.name)
            else:
                await _log(run_id, step, "üîç Running SonarCloud analysis...", pipeline.name)
                await _log(run_id, step, "Results will be available on sonarcloud.io", pipeline.name)
                
                # R√©cup√©rer le token depuis la config
                sonar_token = settings.sonar_token
                if not sonar_token:
                    await _log(run_id, step, "‚ùå ERROR: SONAR_TOKEN not found in .env file", pipeline.name)
                    raise RuntimeError("Missing SONAR_TOKEN - please add it to backend/.env")
                
                # Pr√©parer l'environnement avec le token
                env = os.environ.copy()
                env['SONAR_TOKEN'] = sonar_token
                
                # Commande SonarCloud exacte
                sonar_cmd = [
                    "./mvnw",
                    "verify",
                    "org.sonarsource.scanner.maven:sonar-maven-plugin:sonar",
                    "-Dsonar.projectKey=vulzyun_bfbarchitecture"
                ]
                
                try:
                    await _log(run_id, step, f"Command: {' '.join(sonar_cmd)}", pipeline.name)
                    
                    # Ex√©cuter avec le token dans l'environnement
                    p = subprocess.Popen(
                        sonar_cmd,
                        cwd=str(demo_dir),
                        stdout=subprocess.PIPE,
                        stderr=subprocess.STDOUT,
                        text=True,
                        bufsize=1,
                        env=env
                    )
                    assert p.stdout is not None
                    for line in p.stdout:
                        await _log(run_id, step, line, pipeline.name)
                    rc = p.wait()
                    
                    if rc != 0:
                        await _log(run_id, step, f"‚ùå SonarCloud analysis failed with exit code {rc}", pipeline.name)
                        raise RuntimeError(f"SonarCloud analysis failed ({rc})")
                    
                    await _log(run_id, step, "‚úÖ SonarCloud analysis completed successfully!", pipeline.name)
                    await _log(run_id, step, "üìä View results: https://sonarcloud.io/project/overview?id=vulzyun_bfbarchitecture", pipeline.name)
                    
                except Exception as e:
                    await _log(run_id, step, f"‚ùå SonarCloud analysis error: {e}", pipeline.name)
                    raise
                
                await _step_ok(run_id, step, pipeline.name)

            # STEP: docker build
            step = "docker_build"
            await _step_start(run_id, step, pipeline.name)
            
            build_context = str(demo_dir) if demo_dir.exists() else str(ws)
            await _log(run_id, step, f"Building Docker image: {image_tag}", pipeline.name)
            await _log(run_id, step, f"Build context: {build_context}", pipeline.name)
            
            for line in _run_cmd(["sudo", "docker", "build", "-t", image_tag, build_context]):
                await _log(run_id, step, line, pipeline.name)
            
            await _log(run_id, step, "‚úÖ Docker image built successfully!", pipeline.name)
            await _step_ok(run_id, step, pipeline.name)

            # STEP: cleanup (AVANT d'envoyer la nouvelle image)
            step = "cleanup_old_deploy"
            await _step_start(run_id, step, pipeline.name)

            container_name = sanitized_name
            app_repo = sanitized_name

            # Nettoyer TOUT : TOUS les conteneurs + images de cette app
            cleanup_cmd = f"""
            set -e

            APP_REPO="{app_repo}"
            CONTAINER_NAME="{container_name}"

            echo "[cleanup] Stopping ALL running containers..."
            
            # 1. Arr√™ter TOUS les conteneurs en cours d'ex√©cution (pas seulement ceux de l'app)
            RUNNING_CONTAINERS=$(docker ps -q)
            if [ -n "$RUNNING_CONTAINERS" ]; then
                echo "[cleanup] Found running containers, stopping them..."
                docker stop $RUNNING_CONTAINERS 2>/dev/null || true
                docker rm -f $RUNNING_CONTAINERS 2>/dev/null || true
                echo "[cleanup] All running containers stopped"
            else
                echo "[cleanup] No running containers found"
            fi

            # 2. Nettoyer aussi les conteneurs arr√™t√©s de notre app
            echo "[cleanup] Removing stopped containers for: $CONTAINER_NAME"
            docker ps -aq --filter "name=$CONTAINER_NAME" | xargs -r docker rm -f 2>/dev/null || true

            # 3. Supprimer TOUTES les images de cette app
            echo "[cleanup] Removing all images for: $APP_REPO"
            docker images "$APP_REPO" --format '{{.ID}}' | xargs -r docker rmi -f 2>/dev/null || true

            echo "[cleanup] Done - all containers stopped, ready for fresh deploy"
            """.strip()

            await _log(run_id, step, f"Stopping ALL containers and cleaning images for: {app_repo}", pipeline.name)
            for line in _ssh_exec(DEPLOY_USER, DEPLOY_HOST, DEPLOY_PORT, cleanup_cmd):
                await _log(run_id, step, line, pipeline.name)
            await _step_ok(run_id, step, pipeline.name)

            # STEP: ship image (ssh)
            step = "ship_image_ssh"
            await _step_start(run_id, step, pipeline.name)
            await _log(run_id, step, f"üì¶ Shipping Docker image to {DEPLOY_USER}@{DEPLOY_HOST}", pipeline.name)
            await _log(run_id, step, "This may take several minutes depending on image size...", pipeline.name)
            
            for line in _docker_save_and_load_over_ssh(DEPLOY_USER, DEPLOY_HOST, DEPLOY_PORT, image_tag):
                await _log(run_id, step, line, pipeline.name)
            
            await _log(run_id, step, f"‚úÖ Image {image_tag} successfully transferred!", pipeline.name)
            await _step_ok(run_id, step, pipeline.name)

            # STEP: deploy run
            step = "deploy_run"
            await _step_start(run_id, step, pipeline.name)

            # Lancer le nouveau conteneur
            run_cmd = (
                f"docker run -d "
                f"--name {container_name} "
                f"--restart unless-stopped "
                f"-p 8080:8080 "
                f"{image_tag}"
            )

            await _log(run_id, step, f"Starting new container: {container_name}", pipeline.name)
            for line in _ssh_exec(DEPLOY_USER, DEPLOY_HOST, DEPLOY_PORT, run_cmd):
                await _log(run_id, step, line, pipeline.name)

            await _step_ok(run_id, step, pipeline.name)

            # STEP: healthcheck
            step = "healthcheck"
            await _step_start(run_id, step, pipeline.name)
            healthcheck_url = f"http://{DEPLOY_HOST}:8080/swagger-ui/index.html"
            await _log(run_id, step, f"GET {healthcheck_url}", pipeline.name)
            await _log(run_id, step, "Waiting for container to be ready (timeout: 10s, retries: 30, delay: 2s)...", pipeline.name)
            
            ok, message = _healthcheck(healthcheck_url)
            if not ok:
                await _log(run_id, step, f"‚ùå Healthcheck FAILED: {message}", pipeline.name)
                await _log(run_id, step, "Triggering rollback...", pipeline.name)
                await _step_ok(run_id, step, pipeline.name)
                
                # Rollback if we have a previous commit
                if previous_commit:
                    await _rollback_to_previous(run_id, DEPLOY_USER, DEPLOY_HOST, DEPLOY_PORT, sanitized_name, ws, previous_commit, pipeline.name)
                    
                    # After rollback checkout, rebuild and redeploy
                    await _log(run_id, "rollback", "Now rebuilding and redeploying...", pipeline.name)
                    
                    # Docker build
                    step = "docker_build"
                    await _step_start(run_id, step, pipeline.name)
                    demo_dir = ws / "demo"
                    build_context = str(demo_dir) if demo_dir.exists() else str(ws)
                    await _log(run_id, step, f"Building Docker image: {image_tag}", pipeline.name)
                    
                    for line in _run_cmd(["sudo", "docker", "build", "-t", image_tag, build_context]):
                        await _log(run_id, step, line, pipeline.name)
                    await _step_ok(run_id, step, pipeline.name)
                    
                    # Ship image
                    step = "ship_image_ssh"
                    await _step_start(run_id, step, pipeline.name)
                    for line in _docker_save_and_load_over_ssh(DEPLOY_USER, DEPLOY_HOST, DEPLOY_PORT, image_tag):
                        await _log(run_id, step, line, pipeline.name)
                    await _step_ok(run_id, step, pipeline.name)
                    
                    # Deploy
                    step = "deploy_run"
                    await _step_start(run_id, step, pipeline.name)
                    run_cmd = (
                        f"docker run -d "
                        f"--name {container_name} "
                        f"--restart unless-stopped "
                        f"-p 8080:8080 "
                        f"{image_tag}"
                    )
                    await _log(run_id, step, f"Starting container: {container_name}", pipeline.name)
                    for line in _ssh_exec(DEPLOY_USER, DEPLOY_HOST, DEPLOY_PORT, run_cmd):
                        await _log(run_id, step, line, pipeline.name)
                    await _step_ok(run_id, step, pipeline.name)
                    
                    # Healthcheck final after rollback redeploy
                    step = "healthcheck_rollback"
                    await _step_start(run_id, step, pipeline.name)
                    healthcheck_url = f"http://{DEPLOY_HOST}:8080/swagger-ui/index.html"
                    await _log(run_id, step, f"GET {healthcheck_url}", pipeline.name)
                    await _log(run_id, step, "Checking if rolled-back version is healthy...", pipeline.name)
                    
                    ok_rollback, message_rollback = _healthcheck(healthcheck_url)
                    if ok_rollback:
                        await _log(run_id, step, f"‚úÖ Rollback healthcheck OK! {message_rollback}", pipeline.name)
                        await _step_ok(run_id, step, pipeline.name)
                        await _emit(run_id, {"type": "run_success"}, pipeline.name)
                        await _log(run_id, "success", "üéâ Rollback successful - Previous version is healthy!", pipeline.name)
                        pipeline.status = "success"
                        run.status = RunStatus.success
                    else:
                        await _log(run_id, step, f"‚ùå Rollback healthcheck FAILED: {message_rollback}", pipeline.name)
                        await _step_ok(run_id, step, pipeline.name)
                        await _emit(run_id, {"type": "run_failed", "message": "Rollback deployed but healthcheck failed"}, pipeline.name)
                        await _log(run_id, "error", "‚ùå Rollback deployed but previous version failed healthcheck", pipeline.name)
                        pipeline.status = "failed"
                        run.status = RunStatus.failed
                else:
                    await _emit(run_id, {"type": "run_failed", "message": "Healthcheck failed - no previous version to rollback to"}, pipeline.name)
                    await _log(run_id, "error", "‚ùå Healthcheck failed and no previous version available", pipeline.name)
                    pipeline.status = "failed"
                    run.status = RunStatus.failed
                
                session.add(pipeline)
                run.finished_at = datetime.utcnow()
                session.add(run)
                session.commit()
                return
            else:
                await _log(run_id, step, f"‚úÖ Healthcheck OK! {message}", pipeline.name)
            await _step_ok(run_id, step, pipeline.name)

            # SUCCESS
            await _emit(run_id, {"type": "run_success"}, pipeline.name)
            await _log(run_id, "success", "üéâ Pipeline completed successfully!", pipeline.name)
            pipeline.status = "success"
            session.add(pipeline)

            run.status = RunStatus.success
            run.finished_at = datetime.utcnow()
            session.add(run)
            session.commit()



        except Exception as e:
            # Pipeline FAILED - attempt rollback
            await _emit(run_id, {"type": "run_failed", "message": str(e)}, pipeline.name)
            await _log(run_id, "error", f"‚ùå Pipeline FAILED: {e}", pipeline.name)
            
            # Try to rollback if we have a previous commit
            if previous_commit:
                await _log(run_id, "error", "‚ö†Ô∏è Attempting rollback to previous commit...", pipeline.name)
                await _rollback_to_previous(run_id, DEPLOY_USER, DEPLOY_HOST, DEPLOY_PORT, sanitized_name, ws, previous_commit, pipeline.name)
            else:
                await _log(run_id, "error", "‚ö†Ô∏è No previous commit available for rollback", pipeline.name)
            
            pipeline.status = "failed"
            session.add(pipeline)

            run.status = RunStatus.failed
            run.finished_at = datetime.utcnow()
            session.add(run)
            session.commit()

def run_real_pipeline_bg(run_id: int):
    """
    Wrapper sync pour ex√©cuter le runner async
    depuis FastAPI BackgroundTasks.
    """
    import asyncio
    asyncio.run(run_real_pipeline(run_id))

